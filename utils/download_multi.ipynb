{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24056,"status":"ok","timestamp":1746805700638,"user":{"displayName":"hiroyuki tanabe","userId":"14704902113517717167"},"user_tz":-540},"id":"Poc8IT782nw0","outputId":"4ca17f7a-0894-4da8-c904-335bc6ad72cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":688,"status":"ok","timestamp":1746805703872,"user":{"displayName":"hiroyuki tanabe","userId":"14704902113517717167"},"user_tz":-540},"id":"qSIQ19q03dSP","outputId":"65352c88-bf43-4579-e9e2-5306e9ff5ab9"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/NFFA-SEM/data\n"]}],"source":["%cd \"drive/MyDrive/Colab Notebooks/NFFA-SEM/data\"\n","!ls"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LOg94CVN2szx","executionInfo":{"status":"ok","timestamp":1746806644021,"user_tz":-540,"elapsed":31020,"user":{"displayName":"hiroyuki tanabe","userId":"14704902113517717167"}},"outputId":"52862361-28c7-43bb-c1a1-169be7f41aaa"},"outputs":[{"output_type":"stream","name":"stderr","text":["Biological.tar:   0%|          | 0.00/704M [00:00<?, ?B/s]\n","Biological.tar:   0%|          | 1.05M/704M [00:00<07:42, 1.52MB/s]\n","Biological.tar:   0%|          | 1.57M/704M [00:00<05:20, 2.19MB/s]\n","Biological.tar:   0%|          | 2.62M/704M [00:01<03:01, 3.86MB/s]\n","Biological.tar:   1%|          | 5.24M/704M [00:01<01:20, 8.71MB/s]\n","Biological.tar:   1%|          | 7.86M/704M [00:01<00:55, 12.5MB/s]\n","Biological.tar:   1%|▏         | 10.5M/704M [00:01<00:44, 15.4MB/s]\n","Biological.tar:   2%|▏         | 13.1M/704M [00:01<00:39, 17.6MB/s]\n","Biological.tar:   2%|▏         | 16.3M/704M [00:01<00:33, 20.5MB/s]\n","Biological.tar:   3%|▎         | 18.9M/704M [00:01<00:32, 21.3MB/s]\n","Biological.tar:   3%|▎         | 21.5M/704M [00:01<00:31, 21.8MB/s]\n","Biological.tar:   4%|▎         | 24.6M/704M [00:01<00:28, 23.6MB/s]\n","Biological.tar:   4%|▍         | 27.3M/704M [00:02<00:29, 23.3MB/s]\n","Biological.tar:   4%|▍         | 30.4M/704M [00:02<00:27, 24.5MB/s]\n","Biological.tar:   5%|▍         | 33.0M/704M [00:02<00:27, 24.0MB/s]\n","Biological.tar:   5%|▌         | 35.7M/704M [00:02<00:27, 24.0MB/s]\n","Biological.tar:   5%|▌         | 38.3M/704M [00:02<00:28, 23.6MB/s]\n","Biological.tar:   6%|▌         | 40.9M/704M [00:02<00:28, 23.5MB/s]\n","Biological.tar:   6%|▌         | 43.5M/704M [00:02<00:28, 23.3MB/s]\n","Biological.tar:   7%|▋         | 46.7M/704M [00:02<00:26, 24.7MB/s]\n","Biological.tar:   7%|▋         | 49.3M/704M [00:02<00:27, 24.2MB/s]\n","Biological.tar:   7%|▋         | 52.4M/704M [00:03<00:26, 25.1MB/s]\n","Biological.tar:   8%|▊         | 55.6M/704M [00:03<00:25, 25.8MB/s]\n","Biological.tar:   8%|▊         | 58.2M/704M [00:03<00:25, 25.1MB/s]\n","Biological.tar:   9%|▉         | 61.9M/704M [00:03<00:23, 27.0MB/s]\n","Biological.tar:   9%|▉         | 65.0M/704M [00:03<00:23, 27.4MB/s]\n","Biological.tar:  10%|▉         | 68.2M/704M [00:03<00:23, 27.4MB/s]\n","Biological.tar:  10%|█         | 71.3M/704M [00:03<00:23, 27.4MB/s]\n","Fibres.tar: 100%|██████████| 81.0M/81.0M [00:03<00:00, 22.7MB/s]\n","Biological.tar:  11%|█         | 77.6M/704M [00:03<00:22, 27.5MB/s]\n","Biological.tar:  12%|█▏        | 81.3M/704M [00:04<00:21, 28.7MB/s]\n","Biological.tar:  12%|█▏        | 84.9M/704M [00:04<00:21, 29.3MB/s]\n","Biological.tar:  13%|█▎        | 91.8M/704M [00:04<00:20, 29.5MB/s]\n","Biological.tar:  13%|█▎        | 94.9M/704M [00:04<00:21, 28.4MB/s]\n","Biological.tar:  14%|█▍        | 98.0M/704M [00:04<00:21, 27.7MB/s]\n","Biological.tar:  14%|█▍        | 101M/704M [00:04<00:21, 27.6MB/s] \n","Biological.tar:  15%|█▍        | 104M/704M [00:04<00:21, 27.6MB/s]\n","Biological.tar:  15%|█▌        | 107M/704M [00:05<00:21, 27.8MB/s]\n","Biological.tar:  16%|█▌        | 111M/704M [00:05<00:20, 28.3MB/s]\n","Biological.tar:  16%|█▌        | 114M/704M [00:05<00:21, 27.9MB/s]\n","Biological.tar:  17%|█▋        | 117M/704M [00:05<00:21, 27.9MB/s]\n","Biological.tar:  17%|█▋        | 120M/704M [00:05<00:21, 27.6MB/s]\n","Biological.tar:  18%|█▊        | 123M/704M [00:05<00:21, 27.4MB/s]\n","Biological.tar:  18%|█▊        | 126M/704M [00:05<00:20, 27.7MB/s]\n","Biological.tar:  18%|█▊        | 129M/704M [00:05<00:21, 27.0MB/s]\n","Biological.tar:  19%|█▉        | 133M/704M [00:05<00:21, 26.3MB/s]\n","Biological.tar:  19%|█▉        | 136M/704M [00:06<00:21, 26.2MB/s]\n","Biological.tar:  20%|█▉        | 139M/704M [00:06<00:21, 25.8MB/s]\n","Biological.tar:  20%|██        | 142M/704M [00:06<00:21, 25.6MB/s]\n","Biological.tar:  21%|██        | 145M/704M [00:06<00:22, 25.2MB/s]\n","Biological.tar:  21%|██        | 148M/704M [00:06<00:22, 25.0MB/s]\n","Biological.tar:  22%|██▏       | 152M/704M [00:06<00:21, 26.3MB/s]\n","Biological.tar:  22%|██▏       | 156M/704M [00:06<00:20, 27.0MB/s]\n","Biological.tar:  23%|██▎       | 159M/704M [00:06<00:20, 26.9MB/s]\n","Biological.tar:  23%|██▎       | 162M/704M [00:07<00:20, 26.4MB/s]\n","Biological.tar:  24%|██▎       | 166M/704M [00:07<00:19, 27.2MB/s]\n","Biological.tar:  24%|██▍       | 169M/704M [00:07<00:19, 27.3MB/s]\n","Biological.tar:  25%|██▍       | 172M/704M [00:07<00:19, 26.7MB/s]\n","Biological.tar:  25%|██▍       | 176M/704M [00:07<00:20, 26.3MB/s]\n","Biological.tar:  25%|██▌       | 179M/704M [00:07<00:20, 25.9MB/s]\n","Biological.tar:  26%|██▌       | 183M/704M [00:07<00:19, 27.1MB/s]\n","Biological.tar:  26%|██▋       | 186M/704M [00:07<00:18, 27.8MB/s]\n","Biological.tar:  27%|██▋       | 189M/704M [00:08<00:19, 26.9MB/s]\n","Biological.tar:  27%|██▋       | 192M/704M [00:08<00:19, 26.4MB/s]\n","Biological.tar:  28%|██▊       | 196M/704M [00:08<00:18, 27.2MB/s]\n","Biological.tar:  28%|██▊       | 199M/704M [00:08<00:19, 26.4MB/s]\n","Biological.tar:  29%|██▉       | 202M/704M [00:08<00:19, 26.0MB/s]\n","Biological.tar:  29%|██▉       | 206M/704M [00:08<00:19, 25.6MB/s]\n","Biological.tar:  30%|██▉       | 209M/704M [00:08<00:18, 26.3MB/s]\n","Biological.tar:  30%|███       | 212M/704M [00:08<00:19, 25.8MB/s]\n","Biological.tar:  31%|███       | 215M/704M [00:09<00:19, 25.7MB/s]\n","Biological.tar:  31%|███       | 219M/704M [00:09<00:18, 26.7MB/s]\n","Biological.tar:  32%|███▏      | 222M/704M [00:09<00:18, 26.7MB/s]\n","Biological.tar:  32%|███▏      | 226M/704M [00:09<00:17, 26.9MB/s]\n","Biological.tar:  33%|███▎      | 229M/704M [00:09<00:18, 26.2MB/s]\n","Biological.tar:  33%|███▎      | 232M/704M [00:09<00:19, 24.6MB/s]\n","Biological.tar:  33%|███▎      | 235M/704M [00:09<00:18, 25.7MB/s]\n","Biological.tar:  34%|███▍      | 239M/704M [00:09<00:18, 25.8MB/s]\n","Biological.tar:  34%|███▍      | 242M/704M [00:10<00:17, 26.9MB/s]\n","Biological.tar:  35%|███▍      | 246M/704M [00:10<00:17, 26.5MB/s]\n","Biological.tar:  35%|███▌      | 250M/704M [00:10<00:16, 27.1MB/s]\n","Biological.tar:  36%|███▌      | 253M/704M [00:10<00:17, 26.2MB/s]\n","Biological.tar:  36%|███▋      | 255M/704M [00:10<00:18, 24.6MB/s]\n","Biological.tar:  37%|███▋      | 259M/704M [00:10<00:17, 25.5MB/s]\n","Biological.tar:  37%|███▋      | 263M/704M [00:10<00:16, 26.7MB/s]\n","Biological.tar:  38%|███▊      | 266M/704M [00:11<00:17, 25.6MB/s]\n","Biological.tar:  38%|███▊      | 268M/704M [00:11<00:17, 24.4MB/s]\n","Biological.tar:  39%|███▊      | 272M/704M [00:11<00:17, 24.1MB/s]\n","Biological.tar:  39%|███▉      | 275M/704M [00:11<00:16, 25.8MB/s]\n","Biological.tar:  40%|███▉      | 278M/704M [00:11<00:16, 26.0MB/s]\n","Biological.tar:  40%|███▉      | 281M/704M [00:11<00:17, 24.3MB/s]\n","Biological.tar:  40%|████      | 285M/704M [00:11<00:16, 25.1MB/s]\n","Biological.tar:  41%|████      | 288M/704M [00:11<00:15, 26.1MB/s]\n","Biological.tar:  41%|████▏     | 292M/704M [00:12<00:16, 25.5MB/s]\n","Biological.tar:  42%|████▏     | 295M/704M [00:12<00:16, 25.1MB/s]\n","Biological.tar:  42%|████▏     | 298M/704M [00:12<00:16, 24.9MB/s]\n","Films_Coated_Surface.tar: 100%|██████████| 198M/198M [00:08<00:00, 23.8MB/s]\n","Biological.tar: 100%|██████████| 704M/704M [00:30<00:00, 23.3MB/s]\n"]}],"source":["\"\"\"b2share_downloader.py (v4: 任意数だけダウンロード)\n","-------------------------------------------------\n","* 指定した個数 (`LIMIT_FILES`) だけ .tar を保存\n","* それ以外はスキップ\n","* 進捗バー (tqdm)\n","-------------------------------------------------\n","インストール:\n","    pip install tqdm requests\n","\n","使い方:\n","    LIMIT_FILES を 3 に設定すると先頭 3 件だけ取得します。\n","\"\"\"\n","from __future__ import annotations\n","\n","import hashlib\n","import logging\n","import os\n","import sys\n","import time\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","from typing import Dict, List, Tuple\n","\n","import requests\n","from requests.adapters import HTTPAdapter, Retry\n","from urllib3.exceptions import ProtocolError\n","from http.client import IncompleteRead\n","from tqdm import tqdm\n","\n","# ──────────────────────────  CONFIG  ──────────────────────────\n","BASE_URL = \"https://b2share.eudat.eu\"\n","RECORD_ID = \"80df8606fcdb4b2bae1656f0dc6db8ba\"  # ← 変更可\n","NUM_THREADS = 2          # 並列数\n","MAX_RETRIES = 10         # リトライ回数\n","BACKOFF_START = 10       # 秒\n","CHUNK = 512 * 1024       # 512 KiB\n","LIMIT_FILES = 3          # ← ★ ここを変更 (None なら全ファイル)\n","MIN_EXPECTED_SIZE = 1 * 1024 * 1024  # 1 MiB\n","# ──────────────────────────────────────────────────────────────\n","\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n","    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",")\n","logger = logging.getLogger(\"b2share_dl\")\n","\n","session = requests.Session()\n","retry_strategy = Retry(\n","    total=7,\n","    connect=7,\n","    read=7,\n","    backoff_factor=1,\n","    status_forcelist=[500, 502, 503, 504],\n","    allowed_methods=[\"HEAD\", \"GET\"],\n","    raise_on_status=False,\n",")\n","session.mount(\"https://\", HTTPAdapter(max_retries=retry_strategy))\n","session.mount(\"http://\", HTTPAdapter(max_retries=retry_strategy))\n","\n","\n","def build_true_urls(record_id: str) -> Tuple[List[dict], Dict[str, dict]]:\n","    api_url = f\"{BASE_URL}/api/records/{record_id}\"\n","    logger.info(\"メタデータ取得: %s\", api_url)\n","    r = session.get(api_url, timeout=30)\n","    r.raise_for_status()\n","    record = r.json()\n","    files = record.get(\"files\", record.get(\"entries\", []))\n","    tar_infos, md5_lookup = [], {}\n","    for f in files:\n","        name = f.get(\"key\") or f.get(\"fileName\") or f.get(\"filename\")\n","        bucket = f.get(\"bucket\")\n","        if not name or not bucket:\n","            continue\n","        url = f\"{BASE_URL}/api/files/{bucket}/{name}\"\n","        if name.endswith(\".tar\"):\n","            tar_infos.append({\"name\": name, \"url\": url, \"size\": f.get(\"size\")})\n","        elif name.endswith(\".md5\"):\n","            md5_lookup[name[:-4]] = {\"name\": name, \"url\": url}\n","    return tar_infos, md5_lookup\n","\n","\n","def request_stream(url: str, headers: dict | None = None):\n","    return session.get(url, headers=headers or {}, stream=True, timeout=60, allow_redirects=True)\n","\n","\n","def download_file(url: str, local_path: str) -> bool:\n","    resume_from = os.path.getsize(local_path) if os.path.exists(local_path) else 0\n","    attempt = 0\n","    backoff = BACKOFF_START\n","    while attempt < MAX_RETRIES:\n","        attempt += 1\n","        hdr = {\"Range\": f\"bytes={resume_from}-\"} if resume_from else {}\n","        try:\n","            with request_stream(url, hdr) as r:\n","                r.raise_for_status()\n","                ctype = r.headers.get(\"Content-Type\", \"\").lower()\n","                clen  = int(r.headers.get(\"Content-Length\", \"0\"))\n","                if (\"tar\" not in ctype and \"octet\" not in ctype) or clen < MIN_EXPECTED_SIZE:\n","                    raise ValueError(\"Unexpected response\")\n","                total = clen + resume_from\n","                mode = \"ab\" if resume_from else \"wb\"\n","                with open(local_path, mode) as fp, tqdm(\n","                    total=total,\n","                    initial=resume_from,\n","                    unit=\"B\",\n","                    unit_scale=True,\n","                    desc=os.path.basename(local_path),\n","                ) as bar:\n","                    for chunk in r.iter_content(CHUNK):\n","                        if chunk:\n","                            fp.write(chunk)\n","                            bar.update(len(chunk))\n","            logger.info(\"%s 完了\", local_path)\n","            return True\n","        except (IncompleteRead, ProtocolError, requests.exceptions.ChunkedEncodingError,\n","                requests.exceptions.ConnectionError, requests.exceptions.Timeout) as e:\n","            logger.warning(\"Read/Conn error %s – retry\", e)\n","        except Exception as e:\n","            logger.warning(\"Download error (%s – attempt %d/%d): %s\", local_path, attempt, MAX_RETRIES, e)\n","        time.sleep(backoff)\n","        backoff *= 2\n","        resume_from = os.path.getsize(local_path) if os.path.exists(local_path) else 0\n","    logger.error(\"Failed to download %s\", local_path)\n","    return False\n","\n","\n","def verify_md5(path_: str, expected: str) -> bool:\n","    md5 = hashlib.md5()\n","    with open(path_, \"rb\") as fr:\n","        for chunk in iter(lambda: fr.read(CHUNK), b\"\"):\n","            md5.update(chunk)\n","    ok = md5.hexdigest().lower() == expected.lower()\n","    if not ok:\n","        logger.error(\"MD5 mismatch: %s\", path_)\n","    return ok\n","\n","\n","def task(tar: dict, md5: dict | None):\n","    name = tar[\"name\"]\n","    if not download_file(tar[\"url\"], name):\n","        return name, False\n","    if md5:\n","        try:\n","            exp = session.get(md5[\"url\"], timeout=20).text.strip().split()[0]\n","            if not verify_md5(name, exp):\n","                return name, False\n","        except Exception as e:\n","            logger.error(\"MD5取得失敗 %s: %s\", name, e)\n","            return name, False\n","    return name, True\n","\n","\n","def main(rec_id: str = RECORD_ID):\n","    tar_files, md5_map = build_true_urls(rec_id)\n","    if LIMIT_FILES is not None:\n","        tar_files = tar_files[:LIMIT_FILES]\n","    if not tar_files:\n","        logger.error(\"対象ファイルがありません。\")\n","        sys.exit(1)\n","    logger.info(\"%d 個の .tar をダウンロード開始 (threads=%d)\", len(tar_files), NUM_THREADS)\n","    results = []\n","    with ThreadPoolExecutor(max_workers=NUM_THREADS) as exe:\n","        fut2name = {exe.submit(task, t, md5_map.get(t[\"name\"])): t[\"name\"] for t in tar_files}\n","        for fut in as_completed(fut2name):\n","            name = fut2name[fut]\n","            try:\n","                _, ok = fut.result()\n","                results.append((name, ok))\n","            except Exception as exc:\n","                logger.error(\"Unhandled exception in %s: %s\", name, exc)\n","                results.append((name, False))\n","    ok_cnt = sum(ok for _, ok in results)\n","    logger.info(\"完了: %d / %d 成功\", ok_cnt, len(results))\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":109,"status":"ok","timestamp":1746806703315,"user":{"displayName":"hiroyuki tanabe","userId":"14704902113517717167"},"user_tz":-540},"id":"zAy9wTRJ5-47","outputId":"8dee13b6-0c11-4997-f068-952ba20af752"},"outputs":[{"output_type":"stream","name":"stdout","text":["total 3.6G\n","-rw------- 1 root root 672M May  9 16:04 Biological.tar\n","-rw------- 1 root root  78M May  9 16:03 Fibres.tar\n","-rw------- 1 root root 189M May  9 16:03 Films_Coated_Surface.tar\n","-rw------- 1 root root 2.7G May  9 15:53 Patterned_surface.tar\n"]}],"source":["!ls -lh"]},{"cell_type":"code","source":["\"\"\"verify_md5.py (v1.2)\n","-------------------------------------------------\n","MD5 verifier – now supports **直接ハッシュ指定**\n","  * `-m / --md5 <hex>` で .md5 ファイルを置かずに比較\n","  * 従来通りフォルダ・ファイルを複数渡せる（ハッシュ直指定は 1 ファイル用）\n","-------------------------------------------------\n","Examples\n","========\n","# 1) .md5 ファイルがある場合（従来）\n","!python verify_md5.py Fibres.tar\n","\n","# 2) .md5 が無いのでハッシュ直指定\n","!python verify_md5.py Fibres.tar -m 296cd6be5ac97b2203e807e9552d9aeb\n","\"\"\"\n","from __future__ import annotations\n","\n","import argparse\n","import hashlib\n","import sys\n","from pathlib import Path\n","from typing import Iterable, List, Tuple\n","\n","from tqdm import tqdm\n","\n","CHUNK = 1 << 20  # 1 MiB\n","\n","def compute_md5(path: Path) -> str:\n","    md5 = hashlib.md5()\n","    total = path.stat().st_size\n","    with path.open(\"rb\") as f, tqdm(\n","        total=total,\n","        unit=\"B\",\n","        unit_scale=True,\n","        desc=path.name,\n","        leave=False,\n","    ) as bar:\n","        for chunk in iter(lambda: f.read(CHUNK), b\"\"):\n","            md5.update(chunk)\n","            bar.update(len(chunk))\n","    return md5.hexdigest()\n","\n","def find_md5_file(tar_path: Path) -> Path | None:\n","    for cand in (tar_path.with_suffix(tar_path.suffix + \".md5\"), tar_path.with_suffix(\".md5\")):\n","        if cand.exists():\n","            return cand\n","    return None\n","\n","def parse_expected(md5_file: Path) -> str:\n","    return md5_file.read_text().strip().split()[0]\n","\n","def verify_one(tar_path: Path, expected_hash: str | None = None) -> Tuple[str, bool]:\n","    if expected_hash:\n","        expected = expected_hash\n","    else:\n","        md5file = find_md5_file(tar_path)\n","        if md5file is None:\n","            print(f\"[SKIP] {tar_path.name}: .md5 file not found & no -m given\")\n","            return tar_path.name, False\n","        expected = parse_expected(md5file)\n","    actual = compute_md5(tar_path)\n","    if actual.lower() == expected.lower():\n","        print(f\"[PASS] {tar_path.name}\")\n","        return tar_path.name, True\n","    else:\n","        print(f\"[FAIL] {tar_path.name} – expected {expected}, got {actual}\")\n","        return tar_path.name, False\n","\n","def collect_tars(paths: Iterable[str]) -> List[Path]:\n","    files: List[Path] = []\n","    for p in paths:\n","        p = Path(p)\n","        if p.is_dir():\n","            files.extend(sorted(p.glob(\"*.tar\")))\n","        elif p.suffix == \".tar\":\n","            files.append(p)\n","    # unique order\n","    seen = set()\n","    unique: List[Path] = []\n","    for f in files:\n","        if f not in seen:\n","            seen.add(f)\n","            unique.append(f)\n","    return unique\n","\n","def main():\n","    parser = argparse.ArgumentParser(description=\"MD5 verifier for .tar archives\", add_help=False)\n","    parser.add_argument(\"paths\", nargs=\"*\", default=[\".\"], help=\"directories or .tar files\")\n","    parser.add_argument(\"-m\", \"--md5\", help=\"expected md5 (used when verifying exactly one file)\")\n","    parser.add_argument(\"-h\", \"--help\", action=\"help\", help=\"show this help message and exit\")\n","\n","    args, _ = parser.parse_known_args()\n","\n","    targets = collect_tars(args.paths)\n","    if not targets:\n","        print(\"No .tar files found.\")\n","        sys.exit(1)\n","\n","    # sanity: if md5 string is given, only first target uses it\n","    provided_hash = args.md5\n","    if provided_hash and len(targets) > 1:\n","        print(\"[WARN] --md5 は 1 ファイル用です。先頭ファイルにのみ適用します。\")\n","\n","    passed = 0\n","    for idx, t in enumerate(targets):\n","        exp = provided_hash if idx == 0 else None\n","        _, ok = verify_one(t, exp)\n","        if ok:\n","            passed += 1\n","    print(f\"Summary: {passed}/{len(targets)} passed\")\n","    sys.exit(0 if passed == len(targets) else 2)\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"u-k5V4XmyDgv","executionInfo":{"status":"error","timestamp":1746806854336,"user_tz":-540,"elapsed":45,"user":{"displayName":"hiroyuki tanabe","userId":"14704902113517717167"}},"outputId":"88b9295c-dc00-442f-a42d-f80a35f71838"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["No .tar files found.\n"]},{"output_type":"error","ename":"SystemExit","evalue":"1","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"]}]},{"cell_type":"markdown","source":["https://b2share.eudat.eu/records/80df8606fcdb4b2bae1656f0dc6db8ba"],"metadata":{"id":"AOpFNvij2O3m"}},{"cell_type":"code","source":["!ls -lh"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ZktnlT42CF6","executionInfo":{"status":"ok","timestamp":1746807566426,"user_tz":-540,"elapsed":104,"user":{"displayName":"hiroyuki tanabe","userId":"14704902113517717167"}},"outputId":"e7e335c4-6374-4716-e4aa-218d8b1e969f"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["total 3.6G\n","-rw------- 1 root root 672M May  9 16:04 Biological.tar\n","-rw------- 1 root root  78M May  9 16:03 Fibres.tar\n","-rw------- 1 root root    0 May  9 16:18 Fibres.tar.md5\n","-rw------- 1 root root 189M May  9 16:03 Films_Coated_Surface.tar\n","-rw------- 1 root root 2.7G May  9 15:53 Patterned_surface.tar\n","-rw------- 1 root root 3.8K May  9 16:17 verify_md5.py\n"]}]},{"cell_type":"code","source":["!python verify_md5.py Fibres.tar -m c942e19b8ab221d3c700f44a0c67b306"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l8iURyyty8ZX","executionInfo":{"status":"ok","timestamp":1746807599595,"user_tz":-540,"elapsed":603,"user":{"displayName":"hiroyuki tanabe","userId":"14704902113517717167"}},"outputId":"7dfa7cf6-787f-4380-d121-7947bdf8f142"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["[PASS] Fibres.tar\n","Summary: 1/1 passed\n"]}]},{"cell_type":"code","source":["!python verify_md5.py Biological.tar -m 296cd6be5ac97b2203e807e9552d9aeb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NQazrB330sy0","executionInfo":{"status":"ok","timestamp":1746807666893,"user_tz":-540,"elapsed":2522,"user":{"displayName":"hiroyuki tanabe","userId":"14704902113517717167"}},"outputId":"bda8d199-1490-4335-89dd-5ac23074d303"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["[PASS] Biological.tar\n","Summary: 1/1 passed\n"]}]},{"cell_type":"code","source":["!python verify_md5.py Films_Coated_Surface.tar -m 3c2e6cb914e1864855d9c3805ef47913"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tBysyWd72T3_","executionInfo":{"status":"ok","timestamp":1746807721954,"user_tz":-540,"elapsed":1522,"user":{"displayName":"hiroyuki tanabe","userId":"14704902113517717167"}},"outputId":"96675fef-1d84-4f6e-b9fd-47b58d3c1596"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["[PASS] Films_Coated_Surface.tar\n","Summary: 1/1 passed\n"]}]},{"cell_type":"code","source":["!python verify_md5.py Patterned_surface.tar -m 44d3964bef4ee7e84ba2f99c84036e9c"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xHSIfYUs2oju","executionInfo":{"status":"ok","timestamp":1746807781305,"user_tz":-540,"elapsed":10776,"user":{"displayName":"hiroyuki tanabe","userId":"14704902113517717167"}},"outputId":"f0d21a43-25eb-4c55-a23c-ac31ba342254"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["[PASS] Patterned_surface.tar\n","Summary: 1/1 passed\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"OTlV8kSd20y5"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOjkP6iHG5+DXFUyAA5Z5OY"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}